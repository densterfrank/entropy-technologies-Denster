{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75kX4_ECi8Vu",
        "outputId": "c76c4e9d-59c2-48c9-97c7-ccb063cd8dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all packages\n",
        "import json\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epdbnH4gjD8Z",
        "outputId": "84d15cc6-e1e0-4197-cdc7-2328830fdb70"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_read_txt(filname):\n",
        "  '''\n",
        "  This function read the txt file line by line\n",
        "  '''\n",
        "  with open(filname, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    return lines"
      ],
      "metadata": {
        "id": "8zMUIULDDS44"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_read_json(jsonfilename):\n",
        "  '''\n",
        "  This function read the json file and store both Abbreviation and Synonyms in parameter_list\n",
        "  '''\n",
        "  parameter_list=[]\n",
        "  with open(jsonfilename, 'r') as file:\n",
        "    data = json.load(file)\n",
        "  for eachdic in data:\n",
        "      parameter_list.append(eachdic[\"Abbreviation\"].lower())\n",
        "      for eachsynonyms in eachdic[\"Synonyms\"]:\n",
        "        parameter_list.append(eachsynonyms.lower())\n",
        "  return parameter_list"
      ],
      "metadata": {
        "id": "aVoX83LLD6ob"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def has_numbers(lst):\n",
        "  '''\n",
        "  This function will detect if the line has numbers, if it has number it will give true or false\n",
        "  '''\n",
        "  for item in lst:\n",
        "        if isinstance(item, (int, float)) or (isinstance(item, str) and (item.isdigit() or (item.replace('.', '', 1).isdigit() if '.' in item else False))):\n",
        "            return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "qYv_-3LCjTXj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removeunwated_token(tokens):\n",
        "  '''\n",
        "  This function removes unwanted tokens in the line lik anything in (). Ex (3.5-6.0)\n",
        "  '''\n",
        "  first_index=0\n",
        "  last_index=0\n",
        "  for index,each in enumerate(tokens):\n",
        "    if each ==\"(\":\n",
        "      first_index=index\n",
        "    if each ==\")\":\n",
        "      last_index=index\n",
        "  if last_index!=0:\n",
        "    tokens = tokens[:first_index] + tokens[last_index + 1:]\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "VBOhdf9pjXgA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_unit(word):\n",
        "    '''\n",
        "    This function will detect units, as units start consist of / or %\n",
        "    Exception mg , L etc\n",
        "    '''\n",
        "    pattern = r'[/%]'\n",
        "    if re.search(pattern, word):\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "0q9LvewQjY5e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_word_similar_to_parameters(word, parameter_list, threshold=0.7):\n",
        "    '''\n",
        "    This function uses cosine similary to check is parameter in txt is similar to parameters in json\n",
        "    file, if  similary is greater than 0.75 it returns True\n",
        "    '''\n",
        "    vectorizer = CountVectorizer().fit(parameter_list + [word])\n",
        "    parameter_vectors = vectorizer.transform(parameter_list)\n",
        "    word_vector = vectorizer.transform([word])\n",
        "    cosine_similarities = cosine_similarity(word_vector, parameter_vectors).flatten()\n",
        "    if any(similarity >0.75 for similarity in cosine_similarities):\n",
        "        return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "AqsTAWFmjaZe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_similar_words(word_list, parameter_list, threshold=0.7):\n",
        "    '''\n",
        "    This function will return dic which consit of paramers and its similarity score\n",
        "    '''\n",
        "    vectorizer = CountVectorizer().fit(parameter_list + word_list)\n",
        "    parameter_vectors = vectorizer.transform(parameter_list)\n",
        "    word_list_vectors = vectorizer.transform(word_list)\n",
        "    cosine_similarities = cosine_similarity(word_list_vectors, parameter_vectors)\n",
        "    similar_words_mapping = {}\n",
        "    for i, word in enumerate(word_list):\n",
        "        word_similarity_scores = cosine_similarities[i]\n",
        "        similar_words = [(parameter, similarity) for parameter, similarity in zip(parameter_list, word_similarity_scores) if similarity > threshold]\n",
        "        if similar_words:\n",
        "            similar_words_mapping[word] = similar_words\n",
        "    return similar_words_mapping"
      ],
      "metadata": {
        "id": "VIxCUqdmjbqL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def highest_similarity(similar_words_mapping):\n",
        "    '''\n",
        "    This function detect highest similar score from dic given from find_similar_words function\n",
        "    '''\n",
        "    highest_scores = {}\n",
        "    for word, similarities in similar_words_mapping.items():\n",
        "        max_similarity = max(similarities, key=lambda x: x[1])\n",
        "        highest_scores[word] = max_similarity\n",
        "    return highest_scores"
      ],
      "metadata": {
        "id": "M3p9JT8fjdFi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contains_special(word):\n",
        "    '''\n",
        "    The function contains_special filters data if it contains special symbols such as <, >, |, or -.\n",
        "    '''\n",
        "    if re.search(r'\\b(?:\\d+-\\d+|[<>])\\b', word):\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "BPftaRuM30ao"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contains_and_or_between(text):\n",
        "    '''\n",
        "    This function will detect if lines has and , between words\n",
        "    '''\n",
        "    pattern = r'\\b(?:and|between)\\b'\n",
        "    if re.match(pattern, text):\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "-HaDJd133013"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contains_unwanted(word):\n",
        "    if word in [\"<\",\">\",\"|\"]:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "jYhc0Xx93yS1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify(each_list):\n",
        "  for each_token in each_list:\n",
        "      if contains_and_or_between(each_token) or contains_special(each_token) or contains_unwanted(each_token):\n",
        "        return False\n",
        "  return True"
      ],
      "metadata": {
        "id": "GVj-8OEQFHTV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def makelist_from_lines(lines):\n",
        "  '''\n",
        "  This function make list of all lines from txt\n",
        "  '''\n",
        "  cleaned_list = [line for line in lines if line.strip()]\n",
        "  return cleaned_list"
      ],
      "metadata": {
        "id": "C3CJoY90jgyL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def first_filter(cleaned_list):\n",
        "  '''\n",
        "  This function first_filter contains a data cleaning pipeline that utilizes regex and NLP techniques to remove unwanted lines from a given list of text data. The process involves several steps:\n",
        "\n",
        "Tokenization: Each line of text is converted into tokens.\n",
        "\n",
        "Lowercasing: All tokens are converted into lowercase for consistency.\n",
        "\n",
        "Removal of Unwanted Tokens: Each list of tokens is passed to a function called remove_unwanted_tokens, which removes unwanted data such as (3.5-6.0) from the list of tokens.\n",
        "\n",
        "Filtering Criteria: The filtered lines are determined based on three main conditions:\n",
        "\n",
        "Condition 1: is_word_similar_to_parameters: Checks whether the line has parameters similar to those specified in a JSON file.\n",
        "Condition 2: has_numbers: Determines if the line contains any numbers or floats.\n",
        "Condition 3: detect_unit: Checks if there are any units present in the line, which are identified by characters like / and %.\n",
        "The unwanted lines are removed based on these conditions, and the filtered lines are stored in a list called filtered_data.\n",
        "'''\n",
        "  filtered_data=[]\n",
        "  for line in cleaned_list:\n",
        "      tokens = nltk.word_tokenize(line)\n",
        "      tokens_lower = [token.lower() for token in tokens]\n",
        "      tokens_filtered=removeunwated_token(tokens)\n",
        "      for eachtoken in tokens_lower :\n",
        "        if is_word_similar_to_parameters(eachtoken, parameter_list) and has_numbers(line) and detect_unit(line) :\n",
        "          filtered_data.append(tokens_filtered)\n",
        "  return filtered_data"
      ],
      "metadata": {
        "id": "sLR_oBVIjt2j"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def second_filter(filtered_data):\n",
        "  '''\n",
        "  This function, second_filter, serves as the second layer of filtering in the cleaning process. Each line of data is passed into two separate functions, verify, which filter out unwanted data based on specific conditions.\n",
        "\n",
        "Conditions for Filtering\n",
        "Condition 1: Contains Special Symbols\n",
        "The function contains_special filters data if it contains special symbols such as <, >, |, or -.\n",
        "\n",
        "Condition 2: Contains \"AND\" or \"BETWEEN\"\n",
        "The function contains_and_or_between filters data if it contains the words \"and\" and \"between\" within the line.\n",
        "\n",
        "After passing through these conditions, the filtered data is stored in a list called new_filtered_data.\n",
        "'''\n",
        "  new_filtered_data=[]\n",
        "  for each_list in filtered_data:\n",
        "    if verify(each_list):\n",
        "      new_filtered_data.append(each_list)\n",
        "  return new_filtered_data\n"
      ],
      "metadata": {
        "id": "x1Uaa4jZljXG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_information(new_filtered_data):\n",
        "  '''\n",
        "  This function extract information like parameter, units and value from text using regex, NLP and coine similary method\n",
        "  '''\n",
        "  main_dic=[]\n",
        "  for each_list in new_filtered_data:\n",
        "      sub_dic={}\n",
        "      similar_words_mapping = find_similar_words(each_list, parameter_list)\n",
        "      highest_scores = highest_similarity(similar_words_mapping)\n",
        "      max_limit=0\n",
        "      max_value=0\n",
        "      for i in highest_scores.values():\n",
        "        if i[1]>max_limit:\n",
        "          max_limit=i[1]\n",
        "          max_value=i[0]\n",
        "      sub_dic[\"parameter\"]=max_value\n",
        "\n",
        "      for each_token in each_list:\n",
        "        if  detect_unit(each_token):\n",
        "          index = each_list.index(each_token)\n",
        "          try:\n",
        "            if isinstance(float(each_list[index+1]), (int, float)):\n",
        "              sub_dic[\"value\"]=each_list[index+1]\n",
        "          except:\n",
        "            pass\n",
        "          try:\n",
        "            if isinstance(float(each_list[index-1]), (int, float)):\n",
        "              sub_dic[\"value\"]=each_list[index-1]\n",
        "          except:\n",
        "            pass\n",
        "          sub_dic[\"unit\"]=each_token\n",
        "      main_dic.append(sub_dic)\n",
        "  return main_dic\n"
      ],
      "metadata": {
        "id": "BD-H2fge0kXm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicate_parameter(main_dic):\n",
        "  '''\n",
        "  This function removes duplicate parameter and keeps unique parameter\n",
        "  '''\n",
        "  seen_parameters = set()\n",
        "  Final_dic = []\n",
        "  for item in main_dic:\n",
        "      parameter_name = item.get('parameter')\n",
        "      if parameter_name not in seen_parameters:\n",
        "          Final_dic.append(item)\n",
        "          seen_parameters.add(parameter_name)\n",
        "\n",
        "\n",
        "  return Final_dic\n"
      ],
      "metadata": {
        "id": "jt6KYyfEejV-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filname=\"/content/drive/MyDrive/Entropy-Technologies/0ab9800e-bc9a-4388-aaa2-d4fc05e7d111.txt\"\n",
        "jsonfilename=\"/content/drive/MyDrive/Entropy-Technologies/X1.json\"\n",
        "\n",
        "\n",
        "lines=to_read_txt(filname)\n",
        "parameter_list=to_read_json(jsonfilename)\n",
        "cleaned_list=makelist_from_lines(lines)\n",
        "filtered_data=first_filter(cleaned_list)\n",
        "new_filtered_data=second_filter(filtered_data)\n",
        "main_dic=extract_information(new_filtered_data)\n",
        "Final_dic=remove_duplicate_parameter(main_dic)\n",
        "print(Final_dic)\n",
        "print(len(Final_dic))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFz0DhHiDj8g",
        "outputId": "8259f9da-82de-4622-c589-fdac9b530baf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'parameter': 'tsh', 'value': '0.73', 'unit': 'miIU/L'}, {'parameter': 'ft4', 'value': '15', 'unit': 'pmol/L'}, {'parameter': 'ft3', 'value': '4.8', 'unit': 'pmol/L'}, {'parameter': 'vitamin a', 'value': '108', 'unit': 'nmol/L'}, {'parameter': 'cortisol', 'value': '188', 'unit': 'nmol/L'}, {'parameter': 'iron', 'value': '40', 'unit': 'umol/L'}, {'parameter': \"t'ferrin\", 'value': '24', 'unit': 'umol/L'}, {'parameter': 't. sat.', 'value': '85', 'unit': '%'}, {'parameter': 'ferritin', 'value': '63', 'unit': 'ug/L'}, {'parameter': 'anti-thyroglobulin', 'value': '110', 'unit': 'TU/mL'}, {'parameter': 'cea', 'value': '4.2', 'unit': 'ug/L'}, {'parameter': 'ca 19-9', 'value': '10', 'unit': 'U/mL'}, {'parameter': 'ca125', 'value': '17', 'unit': 'U/mL'}, {'parameter': 'b12', 'value': '252', 'unit': 'pmol/L'}, {'parameter': 'folate', 'value': '23.1', 'unit': 'nmol/L'}, {'parameter': 'na+', 'value': '140', 'unit': 'mmol/L'}, {'parameter': 'cl-', 'value': '106', 'unit': 'mmol/L'}, {'parameter': 'hco3', 'value': '24', 'unit': 'mmol/L'}, {'parameter': 'urea', 'value': '6.5', 'unit': 'mmol/L'}, {'parameter': 'urate', 'value': '0.28', 'unit': 'mmol/L'}, {'parameter': 'bili', 'value': '13', 'unit': 'umol/L'}, {'parameter': 'ast', 'value': '25', 'unit': 'U/L'}, {'parameter': 'alt', 'value': '20', 'unit': 'U/L'}, {'parameter': 'ggt', 'value': '11', 'unit': 'U/L'}, {'parameter': 'phos', 'value': '65', 'unit': 'U/L'}, {'parameter': 'albumin', 'value': '41', 'unit': 'g/L'}, {'parameter': 'glob', 'value': '28', 'unit': 'g/L'}, {'parameter': 'ca', 'value': '2.39', 'unit': 'mmol/L'}, {'parameter': 'po4', 'value': '1.00', 'unit': 'mmol/L'}, {'parameter': 'cpk', 'value': '160', 'unit': 'U/L'}, {'parameter': 'amylase', 'value': '55', 'unit': 'U/L'}, {'parameter': 'mg', 'value': '0.82', 'unit': 'mmol/L'}, {'parameter': 'ca 15-3', 'value': '17', 'unit': 'U/mL'}, {'parameter': 'k test', 'unit': 'MICRO/CULTURE'}, {'parameter': 'ph', 'unit': '/L'}, {'parameter': 'erythrocytes', 'value': '6', 'unit': '/L'}, {'parameter': 'glucose', 'unit': '/L'}, {'parameter': 'esr', 'value': '2', 'unit': 'mm/hr'}, {'parameter': 'hb', 'value': '141', 'unit': 'g/L'}, {'parameter': 'mchc', 'value': '332', 'unit': 'g/L'}, {'parameter': 'rdw', 'value': '12.9', 'unit': '%'}, {'parameter': 'wbc', 'value': '7.2', 'unit': '/L'}, {'parameter': 'neut', 'value': '4.4', 'unit': '/L'}, {'parameter': 'lymph', 'value': '1.8', 'unit': '/L'}, {'parameter': 'mono', 'value': '9', 'unit': '38/L'}, {'parameter': 'eos', 'value': '0.2', 'unit': '=/L'}, {'parameter': 'baso', 'value': '0.1', 'unit': '/L'}, {'parameter': 'plat', 'value': '9', 'unit': '/L'}]\n",
            "48\n"
          ]
        }
      ]
    }
  ]
}